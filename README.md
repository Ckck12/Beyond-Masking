# Beyond-Masking: Landmark-Guided Self-Distillation for Multimodal Deepfake Detection

[![Conference](https://img.shields.io/badge/CIKM-2025-blue)](https://www.cikm2025.org/)


Official PyTorch implementation for the CIKM 2025 paper: **"Beyond-Masking: Landmark-Guided Self-Distillation for Multimodal Deepfake Detection"**.

## ğŸ“– Overview

This paper proposes a novel multimodal deepfake detection framework that leverages video, audio, and facial landmarks. We introduce two core mechanisms to maximize detection performance:

1.  **Landmark-Based Distillation (LBD)**: A knowledge distillation technique that uses the geometric information of facial landmarks as a 'teacher' signal. This forces the video and audio feature extractors to focus on facial regions crucial for deepfake detection, such as the eyes, mouth, and facial contours.
2.  **Multimodal Temporal Information Alignment (MTIA)**: Based on Transformers and Cross-Attention, this module captures and synchronizes features by identifying subtle misalignments or asynchronies between the time-varying video and audio streams.

<!-- ![Figure 2: Overview of our proposed framework.](https://i.imgur.com/your_image_link.png) *<p align="center">Figure 1: Overview of our proposed framework.</p>* -->

---
## âš™ï¸ Setup

#### 1. Clone the repository
```bash
git clone [https://github.com/your_username/Beyond-Masking.git](https://github.com/Ckck12/Beyond-Masking.git)
cd Beyond-Masking
```
python 3.9
pytorch 2.6+cu124
CUDA verion 12.4

## âš™ï¸ Dataset Preparation
This project supports multiple public deepfake datasets. The corresponding data loaders can be found in the loaders/ directory.

#### 1. Download Datasets: 
Download the original datasets (e.g., FakeAVCeleb, KoDF, FaceForensics++) and place them in a designated directory (e.g., /media/NAS/DATASET/). -> â€»All the dataset has different file_structure that you have to rearrange code or folder structure fitting for your structure.â€»

#### 2. Run Preprocessing: 
Before training, you must preprocess each dataset to extract cropped face videos, audio waveforms (.wav), and facial landmarks (.npy).
##### 1. Prepare video data -> 2. Prepare landmark data (MediaPipe) -> 3. Prepare audio data.

#### 3. Directory Sturcture for Preprocessed Data (Examples)

``` bash
/media/NAS/DATASET/FakeAVCeleb_v1.2/
â”œâ”€â”€ FakeAVCeleb_metadata.csv
â””â”€â”€ landmark_features/
    â””â”€â”€ features_mediapipe/
        â”œâ”€â”€ FakeVideo-FakeAudio/
        â”‚   â””â”€â”€ African/
        â”‚       â””â”€â”€ men/
        â”‚           â””â”€â”€ id00076/
        â”‚               â””â”€â”€ 00109_2_id00701_wavtolip/  
        â”‚                   â”œâ”€â”€ cropped_video.mp4      # Cropped face video
        â”‚                   â”œâ”€â”€ cropped_video.wav      # Extracted audio
        â”‚                   â””â”€â”€ landmarks.npy          # Landmark data
        â””â”€â”€ RealVideo-RealAudio/
            â””â”€â”€ African/
                â””â”€â”€ men/
                    â””â”€â”€ id00076/
                        â””â”€â”€ 00109/                     
                            â”œâ”€â”€ cropped_video.mp4
                            â”œâ”€â”€ cropped_video.wav
                            â””â”€â”€ landmarks.npy
```                            

preparing all the modality datas in advance is much more efficient.



## âš™ï¸ Training
Training scripts for each dataset are provided as run_*.sh files in the root directory. These scripts are pre-configured with the optimized hyperparameters reported in our paper.

To start training on the FakeAVCeleb dataset, for example, simply run the corresponding script:

```bash
bash run_fakeavceleb.sh
```

You can easily customize the training by modifying the variables (e.g., BATCH_SIZE, LEARNING_RATE, FEATURE_DIM) at the top of each script. The training progress, logs, and best model checkpoints will be saved to the saved_models/ directory.
