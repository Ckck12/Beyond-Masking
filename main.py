#!/usr/bin/env python3

import argparse
import sys
from pathlib import Path
from datasets import create_dataset_from_args, DatasetConfig, ProcessingConfig
from model import LandmarkPredictor, ModelConfig, create_model, get_model_info
from trainer import (
    TrainingConfig, ModelTrainer, get_device, 
    set_all_seeds, print_system_info, validate_config
)
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim


class MainTrainer:
    def __init__(self, args):
        self.args = args
        self.device = get_device(args.device)
        
        # ì‹œë“œ ì„¤ì •
        set_all_seeds(args.seed)
        
        # ì„¤ì • ìƒì„±
        self.dataset_config = self._create_dataset_config()
        self.processing_config = self._create_processing_config()
        self.model_config = self._create_model_config()
        self.training_config = self._create_training_config()
        
        print("=" * 60)
        print("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        
    def _create_dataset_config(self) -> DatasetConfig:
        return DatasetConfig(
            deepspeak_base_dir=self.args.deepspeak_data_dir,
            kodf_features_root_dir=self.args.kodf_data_dir,
            fakeavceleb_dir=self.args.fakeavceleb_dir,
            dfdc_dir=self.args.dfdc_dir,
            deepfaketimit_dir=self.args.deepfaketimit_dir,
            faceforensics_dir=self.args.faceforensics_dir,
            load_deepspeak_real=self.args.load_deepspeak_real,
            load_deepspeak_fake=self.args.load_deepspeak_fake,
            load_kodf_real=self.args.load_kodf_real,
            load_kodf_fake=self.args.load_kodf_fake,
            load_fakeavceleb=self.args.load_fakeavceleb,
            load_dfdc_real=self.args.load_dfdc_real,
            load_dfdc_fake=self.args.load_dfdc_fake,
            load_deepfaketimit_fake=self.args.load_deepfaketimit_fake,
            load_faceforensics_real=self.args.load_faceforensics_real,
            load_faceforensics_fake=self.args.load_faceforensics_fake,
        )
    
    def _create_processing_config(self) -> ProcessingConfig:
        return ProcessingConfig(
            desired_num_frames=self.args.input_frames,
            video_size=(224, 224),
            landmark_dim=478 * 2
        )
    
    def _create_model_config(self) -> ModelConfig:
        return ModelConfig(
            input_frames=self.args.input_frames,
            video_feature_dim=self.args.video_feature_dim,
            recon_target_dim=self.args.recon_target_dim,
            dropout_rate=self.args.dropout_rate
        )
    
    def _create_training_config(self) -> TrainingConfig:
        return TrainingConfig(
            num_epochs=self.args.num_epochs,
            batch_size=self.args.batch_size,
            learning_rate=self.args.learning_rate,
            recon_loss_weight=self.args.recon_loss_weight,
            num_workers=self.args.num_workers,
            device=str(self.device),
            seed=self.args.seed,
            save_best_model=True,
            model_save_dir=self.args.model_save_dir,
            early_stopping_patience=self.args.early_stopping_patience,
            log_every_n_batches=self.args.log_every_n_batches
        )
    
    def create_datasets(self):
        print("\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        
        datasets = {}
        for partition in ['train', 'val', 'test']:
            print(f"\n--- {partition.upper()} ë°ì´í„°ì…‹ ---")
            
            try:
                # ë¦¬íŒ©í† ë§ëœ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±
                dataset = create_dataset_from_args(self.args, partition)
                
                if len(dataset) == 0:
                    print(f"âš ï¸  {partition} ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    datasets[partition] = None
                else:
                    print(f"âœ… {partition} ë°ì´í„°ì…‹: {len(dataset)} ìƒ˜í”Œ")
                    datasets[partition] = dataset
                    
                    # ë°ì´í„° ì „ì²˜ë¦¬ (ì˜µì…˜)
                    if self.args.preload_data and dataset:
                        print(f"ğŸ”„ {partition} ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
                        dataset.preload_data(batch_size=100)
                        
            except Exception as e:
                print(f"âŒ {partition} ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
                datasets[partition] = None
        
        return datasets['train'], datasets['val'], datasets['test']
    
    def create_dataloaders(self, train_dataset, val_dataset, test_dataset):
        print("\nğŸ”„ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
        
        def create_loader(dataset, partition, shuffle=False, drop_last=False):
            if dataset is None or len(dataset) == 0:
                return None
                
            return DataLoader(
                dataset,
                batch_size=self.args.batch_size,
                shuffle=shuffle,
                drop_last=drop_last,
                num_workers=self.args.num_workers,
                pin_memory=True,
                persistent_workers=True if self.args.num_workers > 0 else False
            )
        
        train_loader = create_loader(train_dataset, 'train', shuffle=True, drop_last=True)
        val_loader = create_loader(val_dataset, 'val', shuffle=False, drop_last=False)
        test_loader = create_loader(test_dataset, 'test', shuffle=False, drop_last=False)
        
        if train_loader is None:
            raise ValueError("âŒ í•™ìŠµ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ê²½ë¡œì™€ ë¡œë”© ì˜µì…˜ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        # ë°°ì¹˜ ìˆ˜ ì¶œë ¥
        batch_info = [f"train: {len(train_loader)}"]
        if val_loader:
            batch_info.append(f"val: {len(val_loader)}")
        if test_loader:
            batch_info.append(f"test: {len(test_loader)}")
        
        print(f"ğŸ“Š DataLoader ë°°ì¹˜ ìˆ˜ â†’ {', '.join(batch_info)}")
        
        return train_loader, val_loader, test_loader
    
    def create_model(self):
        """ëª¨ë¸ ìƒì„±"""
        print("\nğŸ§  ëª¨ë¸ ìƒì„± ì¤‘...")
        
        model = create_landmark_predictor(
            input_frames=self.model_config.input_frames,
            video_feature_dim=self.model_config.video_feature_dim,
            recon_target_dim=self.model_config.recon_target_dim,
            dropout_rate=self.model_config.dropout_rate
        ).to(self.device)
        
        print(get_model_info(model))
        return model
    
    def create_optimizer_and_criterion(self, model):
        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.Adam(model.parameters(), lr=self.args.learning_rate)
        
        print(f" ì†ì‹¤í•¨ìˆ˜: BCEWithLogitsLoss")
        print(f" ì˜µí‹°ë§ˆì´ì €: Adam (lr={self.args.learning_rate})")
        
        return optimizer, criterion
    
    def run_training(self):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
            if self.args.verbose:
                print_system_info()
            
            # ì„¤ì • ê²€ì¦
            if not validate_config(self.training_config):
                raise ValueError("ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")
            
            # 1. ë°ì´í„°ì…‹ ìƒì„±
            train_dataset, val_dataset, test_dataset = self.create_datasets()
            
            # 2. ë°ì´í„°ë¡œë” ìƒì„±
            train_loader, val_loader, test_loader = self.create_dataloaders(
                train_dataset, val_dataset, test_dataset
            )
            
            # 3. ëª¨ë¸ ìƒì„±
            model = self.create_model()
            
            # 4. ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤í•¨ìˆ˜
            optimizer, criterion = self.create_optimizer_and_criterion(model)
            
            # 5. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
            print(f"\nğŸƒ í•™ìŠµ ì‹œì‘!")
            print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.args.model_save_dir}")
            
            trainer = ModelTrainer(
                model=model,
                criterion=criterion,
                optimizer=optimizer,
                config=self.training_config,
                device=self.device
            )
            
            trainer.train(train_loader, val_loader, test_loader)
            
            print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
            if trainer.best_model_path:
                print(f" ìµœê³  ì„±ëŠ¥: {trainer.best_model_path}")
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)
        except Exception as e:
            print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


def create_argument_parser():
    parser = argparse.ArgumentParser(
        description="ë©€í‹°ëª¨ë‹¬ ë”¥í˜ì´í¬ íƒì§€ ëª¨ë¸ í•™ìŠµ",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    training_group = parser.add_argument_group('ğŸƒ í•™ìŠµ ì„¤ì •')
    training_group.add_argument('--num_epochs', type=int, default=10, 
                               help='í•™ìŠµ ì—í¬í¬ ìˆ˜')
    training_group.add_argument('--batch_size', type=int, default=16,
                               help='ë°°ì¹˜ í¬ê¸°')
    training_group.add_argument('--learning_rate', type=float, default=1e-3,
                               help='í•™ìŠµë¥ ')
    training_group.add_argument('--recon_loss_weight', type=float, default=0.5,
                               help='ì¬êµ¬ì„± ì†ì‹¤ ê°€ì¤‘ì¹˜')
    training_group.add_argument('--early_stopping_patience', type=int, default=None,
                               help='ì¡°ê¸° ì¢…ë£Œ patience (None=ë¹„í™œì„±í™”)')
    training_group.add_argument('--log_every_n_batches', type=int, default=10,
                               help='ë¡œê·¸ ì¶œë ¥ ì£¼ê¸° (ë°°ì¹˜ ë‹¨ìœ„)')
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    system_group = parser.add_argument_group('ğŸ’» ì‹œìŠ¤í…œ ì„¤ì •')
    system_group.add_argument('--device', type=str, default='auto',
                             help='ë””ë°”ì´ìŠ¤ (auto, cpu, cuda, cuda:0, etc.)')
    system_group.add_argument('--num_workers', type=int, default=4,
                             help='DataLoader ì›Œì»¤ ìˆ˜')
    system_group.add_argument('--num_workers_preload', type=int, default=8,
                             help='ë°ì´í„° ì „ì²˜ë¦¬ ì›Œì»¤ ìˆ˜')
    system_group.add_argument('--seed', type=int, default=42,
                             help='ëœë¤ ì‹œë“œ')
    system_group.add_argument('--verbose', action='store_true',
                             help='ìƒì„¸ ì¶œë ¥ ëª¨ë“œ')
    
    # ë°ì´í„° ì„¤ì •
    data_group = parser.add_argument_group('ğŸ“‚ ë°ì´í„° ì„¤ì •')
    data_group.add_argument('--preload_data', action='store_true',
                           help='ë°ì´í„° ì‚¬ì „ ë¡œë”© í™œì„±í™”')
    data_group.add_argument('--cache_dir', type=str, default="cache",
                           help='ìºì‹œ ë””ë ‰í† ë¦¬')
    data_group.add_argument('--model_save_dir', type=str, default="saved_models",
                           help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    # ëª¨ë¸ ì„¤ì •
    model_group = parser.add_argument_group('ğŸ§  ëª¨ë¸ ì„¤ì •')
    model_group.add_argument('--input_frames', type=int, default=60,
                            help='ì…ë ¥ ë¹„ë””ì˜¤ í”„ë ˆì„ ìˆ˜')
    model_group.add_argument('--video_feature_dim', type=int, default=256,
                            help='ë¹„ë””ì˜¤ íŠ¹ì§• ì°¨ì›')
    model_group.add_argument('--recon_target_dim', type=int, default=956,
                            help='ëœë“œë§ˆí¬ ì¬êµ¬ì„± ëª©í‘œ ì°¨ì› (478*2)')
    model_group.add_argument('--dropout_rate', type=float, default=0.3,
                            help='ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨')
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    path_group = parser.add_argument_group('ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ')
    path_group.add_argument('--deepspeak_data_dir', type=str,
                           default="/media/NAS/DATASET/deepspeak/",
                           help='DeepSpeak ë°ì´í„°ì…‹ ê²½ë¡œ')
    path_group.add_argument('--kodf_data_dir', type=str,
                           default="/media/NAS/DATASET/KoDF/features_mtcnn/",
                           help='KoDF ë°ì´í„°ì…‹ ê²½ë¡œ')
    path_group.add_argument('--fakeavceleb_dir', type=str,
                           default="/media/NAS/DATASET/FakeAVCeleb_v1.2/",
                           help='FakeAVCeleb ë°ì´í„°ì…‹ ê²½ë¡œ')
    path_group.add_argument('--dfdc_dir', type=str,
                           default="/media/NAS/DATASET/DFDC-Official/",
                           help='DFDC ë°ì´í„°ì…‹ ê²½ë¡œ')
    path_group.add_argument('--deepfaketimit_dir', type=str,
                           default="/media/NAS/DATASET/DeepfakeTIMIT/",
                           help='DeepfakeTIMIT ë°ì´í„°ì…‹ ê²½ë¡œ')
    path_group.add_argument('--faceforensics_dir', type=str,
                           default="/media/NAS/DATASET/FaceForensics/",
                           help='FaceForensics++ ë°ì´í„°ì…‹ ê²½ë¡œ')
    
    # ë°ì´í„°ì…‹ ë¡œë”© ì˜µì…˜ (ê°„ì†Œí™”)
    dataset_group = parser.add_argument_group('ğŸ“‹ ë°ì´í„°ì…‹ ë¡œë”© ì˜µì…˜')
    
    # DeepSpeak
    dataset_group.add_argument('--load_deepspeak_real', action='store_true',
                              help='DeepSpeak ì‹¤ì œ ë°ì´í„° ë¡œë“œ')
    dataset_group.add_argument('--load_deepspeak_fake', action='store_true',
                              help='DeepSpeak ê°€ì§œ ë°ì´í„° ë¡œë“œ')
    
    # KoDF
    dataset_group.add_argument('--load_kodf_real', action='store_true',
                              help='KoDF ì‹¤ì œ ë°ì´í„° ë¡œë“œ')
    dataset_group.add_argument('--load_kodf_fake', action='store_true',
                              help='KoDF ê°€ì§œ ë°ì´í„° ë¡œë“œ')
    
    # ê¸°íƒ€ ë°ì´í„°ì…‹
    dataset_group.add_argument('--load_fakeavceleb', action='store_true',
                              help='FakeAVCeleb ë°ì´í„° ë¡œë“œ')
    dataset_group.add_argument('--load_dfdc_real', action='store_true',
                              help='DFDC ì‹¤ì œ ë°ì´í„° ë¡œë“œ')
    dataset_group.add_argument('--load_dfdc_fake', action='store_true',
                              help='DFDC ê°€ì§œ ë°ì´í„° ë¡œë“œ')
    dataset_group.add_argument('--load_deepfaketimit_fake', action='store_true',
                              help='DeepfakeTIMIT ê°€ì§œ ë°ì´í„° ë¡œë“œ')
    dataset_group.add_argument('--load_faceforensics_real', action='store_true',
                              help='FaceForensics++ ì‹¤ì œ ë°ì´í„° ë¡œë“œ')
    dataset_group.add_argument('--load_faceforensics_fake', action='store_true',
                              help='FaceForensics++ ê°€ì§œ ë°ì´í„° ë¡œë“œ')
    
    return parser


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # í•™ìŠµ ì‹¤í–‰
    trainer = MainTrainer(args)
    trainer.run_training()


if __name__ == "__main__":
    main()